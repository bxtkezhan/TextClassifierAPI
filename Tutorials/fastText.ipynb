{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python3\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "import  keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gzip, pickle\n",
    "\n",
    "\n",
    "class Text2Dataset:\n",
    "    def __init__(self, wordNgrams=1, label_prefix='__label__', minCount=1):\n",
    "        self.wordNgrams = wordNgrams\n",
    "        self.label_prefix = label_prefix\n",
    "        self.minCount = minCount\n",
    "\n",
    "        self.word2idx = None\n",
    "        self.words2idx = None\n",
    "        self.label2idx = None\n",
    "        self.idx2label = None\n",
    "        self.train_X = None\n",
    "        self.train_y = None\n",
    "        self.max_features = None\n",
    "        self.token_indice = None\n",
    "\n",
    "    def create_ngram_set(self, input_list, ngram_value=2):\n",
    "        return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "    def add_ngram(self, sequences):\n",
    "        new_sequences = []\n",
    "        for input_list in sequences:\n",
    "            new_list = input_list[:]\n",
    "            for ngram_value in range(2, self.wordNgrams + 1):\n",
    "                for i in range(len(new_list) - ngram_value + 1):\n",
    "                    ngram = tuple(new_list[i:i + ngram_value])\n",
    "                    if ngram in self.token_indice:\n",
    "                        new_list.append(self.token_indice[ngram])\n",
    "            new_sequences.append(new_list)\n",
    "\n",
    "        return new_sequences\n",
    "\n",
    "    def text2List(self, text_path):\n",
    "        with open(text_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        getLabel = lambda line: [words.strip() for words in line.split(',') if self.label_prefix in words][0]\n",
    "        getWords = lambda line: (','.join([words for words in line.split(',') if self.label_prefix not in words])\n",
    "                                 .strip().replace('\\n', ''))\n",
    "\n",
    "        label_list = [getLabel(line) for line in lines]\n",
    "        words_list = [getWords(line) for line in lines]\n",
    "\n",
    "        return words_list, label_list\n",
    "\n",
    "    def loadTrain(self, text_path):\n",
    "        words_list, label_list = self.text2List(text_path)\n",
    "\n",
    "        label_words_dict = {}\n",
    "        for label, words in zip(label_list, words_list):\n",
    "            if len(words) > self.minCount:\n",
    "                if label in label_words_dict:\n",
    "                    label_words_dict[label].append(words)\n",
    "                else:\n",
    "                    label_words_dict[label] = []\n",
    "\n",
    "        self.label2idx = {label: idx for idx, label in enumerate(label_words_dict)}\n",
    "        self.idx2label = {self.label2idx[label]: label for label in self.label2idx}\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(set(' '.join(words_list).split()))}\n",
    "        self.words2idx = lambda words: [self.word2idx[word] for word in words.split() if word in self.word2idx]\n",
    "\n",
    "        self.train_X = [self.words2idx(words) for words in words_list]\n",
    "        self.train_y = [self.label2idx[label] for label in label_list]\n",
    "        self.max_features = len(self.word2idx)\n",
    "\n",
    "        if self.wordNgrams > 1:\n",
    "            print('Adding {}-gram features'.format(self.wordNgrams))\n",
    "            ngram_set = set()\n",
    "            for input_list in self.train_X:\n",
    "                for i in range(2, self.wordNgrams + 1):\n",
    "                    set_of_ngram = self.create_ngram_set(input_list, ngram_value=i)\n",
    "                    ngram_set.update(set_of_ngram)\n",
    "\n",
    "            start_index = self.max_features + 1\n",
    "            self.token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "            indice_token = {self.token_indice[k]: k for k in self.token_indice}\n",
    "\n",
    "            self.max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "            self.train_X = self.add_ngram(self.train_X)\n",
    "\n",
    "        return self.train_X, self.train_y\n",
    "\n",
    "    def loadTest(self, text_path):\n",
    "        words_list, label_list = self.text2List(text_path)\n",
    "\n",
    "        test_X = [self.words2idx(words) for words in words_list]\n",
    "        test_y = [self.label2idx[label] for label in label_list]\n",
    "        test_X = self.add_ngram(test_X)\n",
    "\n",
    "        return test_X, test_y\n",
    "\n",
    "class FastText:\n",
    "    def __init__(self, wordNgrams=1, label_prefix='__label__', minCount=1, args=None):\n",
    "        if args is None:\n",
    "            self.text2Dataset = Text2Dataset(wordNgrams, label_prefix, minCount)\n",
    "\n",
    "            self.max_features = None\n",
    "            self.maxlen = None\n",
    "            self.batch_size = None\n",
    "            self.embedding_dims = None\n",
    "            self.epochs = None\n",
    "            self.lr = None\n",
    "            self.num_classes = None\n",
    "            self.model = None\n",
    "        else:\n",
    "            (wordNgrams, label_prefix, minCount, word2idx, label2idx, token_indice,\n",
    "             self.max_features, self.maxlen, self.batch_size, self.embedding_dims,\n",
    "             self.epochs, self.lr, self.num_classes, model_weights) = args\n",
    "\n",
    "            self.text2Dataset = Text2Dataset(wordNgrams, label_prefix, minCount)\n",
    "            self.text2Dataset.words2idx = lambda words: [word2idx[word] for word in words.split() if word in word2idx]\n",
    "            self.text2Dataset.label2idx = label2idx\n",
    "            self.text2Dataset.idx2label = {label2idx[label]: label for label in label2idx}\n",
    "            self.text2Dataset.token_indice = token_indice\n",
    "            self.model = self.build_model(model_weights)\n",
    "\n",
    "    def precision(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    def recall(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def build_model(self, weights=None):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=keras.optimizers.Adam(lr=self.lr),\n",
    "                metrics=[self.precision, self.recall])\n",
    "\n",
    "        if weights is not None:\n",
    "            model.set_weights(weights)\n",
    "        return model\n",
    "\n",
    "    def train(self, text_path, maxlen=400, batch_size=32, embedding_dims=100, epochs=5, lr=0.001, verbose=1):\n",
    "        train_X, train_y = self.text2Dataset.loadTrain(text_path)\n",
    "\n",
    "        self.max_features = self.text2Dataset.max_features\n",
    "        self.maxlen = maxlen\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.num_classes = len(set(train_y))\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        train_X = sequence.pad_sequences(train_X, maxlen=self.maxlen)\n",
    "        train_Y = to_categorical(train_y, self.num_classes)\n",
    "\n",
    "        self.model.fit(train_X, train_Y, batch_size=self.batch_size, epochs=self.epochs, verbose=verbose)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def test(self, text_path, verbose=1):\n",
    "        test_X, test_y = self.text2Dataset.loadTest(text_path)\n",
    "        test_X = sequence.pad_sequences(test_X, maxlen=self.maxlen)\n",
    "        test_Y = to_categorical(test_y, self.num_classes)\n",
    "        \n",
    "        c, p, r = self.model.evaluate(test_X, test_Y, batch_size=self.batch_size, verbose=verbose)\n",
    "\n",
    "        print(\"N\\t\" + str(len(test_y)))\n",
    "        print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "        print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "        return str(len(test_y)), p, r\n",
    "\n",
    "    def save_model(self, path):\n",
    "        args = (self.text2Dataset.wordNgrams, self.text2Dataset.label_prefix,\n",
    "                self.text2Dataset.minCount, self.text2Dataset.word2idx,\n",
    "                self.text2Dataset.label2idx, self.text2Dataset.token_indice,\n",
    "                self.max_features, self.maxlen, self.batch_size, self.embedding_dims,\n",
    "                self.epochs, self.lr, self.num_classes, self.model.get_weights())\n",
    "        with gzip.open(path, 'wb') as f:\n",
    "            pickle.dump(args, f)\n",
    "\n",
    "    def predict(self, text, k=1):\n",
    "        text = ','.join([words for words in text.split(',')]).strip().replace('\\n', '')\n",
    "        X = self.text2Dataset.words2idx(text)\n",
    "        X = self.text2Dataset.add_ngram([X])\n",
    "        X = sequence.pad_sequences(X, maxlen=self.maxlen)\n",
    "        predict = self.model.predict(X).flatten()\n",
    "        results = [(self.text2Dataset.idx2label[idx], predict[idx]) for idx in range(len(predict))]\n",
    "        return sorted(results, key=lambda item: item[1], reverse=True)[:k]\n",
    "\n",
    "def train_supervised(input, lr=0.01, dim=100, epoch=5, minCount=1, wordNgrams=1, label='__label__', verbose=1, maxlen=400):\n",
    "    fastText = FastText(wordNgrams=wordNgrams, label_prefix=label, minCount=minCount)\n",
    "    fastText.train(text_path=input, maxlen=maxlen, embedding_dims=dim, epochs=epoch, lr=lr, verbose=verbose)\n",
    "    return fastText\n",
    "\n",
    "def load_model(path):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    fastText = FastText(args=args)\n",
    "    return fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.6388 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 166us/step - loss: 2.5562 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 155us/step - loss: 2.4791 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 177us/step - loss: 2.4152 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 198us/step - loss: 2.3556 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 195us/step - loss: 2.2861 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 174us/step - loss: 2.2011 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 173us/step - loss: 2.1041 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 172us/step - loss: 1.9884 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 160us/step - loss: 1.8475 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 176us/step - loss: 1.7024 - precision: 0.6400 - recall: 0.0200  \n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 183us/step - loss: 1.5516 - precision: 1.0000 - recall: 0.0700\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 179us/step - loss: 1.3951 - precision: 0.9600 - recall: 0.1500\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 183us/step - loss: 1.2324 - precision: 1.0000 - recall: 0.2800\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 149us/step - loss: 1.0797 - precision: 1.0000 - recall: 0.3400\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 189us/step - loss: 0.9379 - precision: 0.9600 - recall: 0.4600\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.8174 - precision: 1.0000 - recall: 0.5400\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 156us/step - loss: 0.7291 - precision: 1.0000 - recall: 0.6300\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 161us/step - loss: 0.6483 - precision: 1.0000 - recall: 0.6700\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 179us/step - loss: 0.5662 - precision: 1.0000 - recall: 0.7300\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 161us/step - loss: 0.5034 - precision: 1.0000 - recall: 0.7300\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 224us/step - loss: 0.4487 - precision: 1.0000 - recall: 0.7400\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 157us/step - loss: 0.4033 - precision: 1.0000 - recall: 0.7600\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 183us/step - loss: 0.3647 - precision: 1.0000 - recall: 0.7900\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.3301 - precision: 1.0000 - recall: 0.8300\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 159us/step - loss: 0.2939 - precision: 1.0000 - recall: 0.8700\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.2653 - precision: 1.0000 - recall: 0.8800\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.2432 - precision: 1.0000 - recall: 0.9000\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 157us/step - loss: 0.2234 - precision: 1.0000 - recall: 0.9100\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 152us/step - loss: 0.2054 - precision: 1.0000 - recall: 0.9200\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 190us/step - loss: 0.1883 - precision: 1.0000 - recall: 0.9200\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.1709 - precision: 1.0000 - recall: 0.9300\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 166us/step - loss: 0.1570 - precision: 1.0000 - recall: 0.9300\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 169us/step - loss: 0.1456 - precision: 1.0000 - recall: 0.9400\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 173us/step - loss: 0.1360 - precision: 1.0000 - recall: 0.9500\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 184us/step - loss: 0.1256 - precision: 1.0000 - recall: 0.9500\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 164us/step - loss: 0.1157 - precision: 1.0000 - recall: 0.9700\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 191us/step - loss: 0.1067 - precision: 1.0000 - recall: 0.9800\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 169us/step - loss: 0.1009 - precision: 1.0000 - recall: 0.9800\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 159us/step - loss: 0.0927 - precision: 1.0000 - recall: 0.9800\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0858 - precision: 1.0000 - recall: 0.9800\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 172us/step - loss: 0.0807 - precision: 1.0000 - recall: 0.9800\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 163us/step - loss: 0.0770 - precision: 1.0000 - recall: 0.9800\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 166us/step - loss: 0.0715 - precision: 1.0000 - recall: 0.9900\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 162us/step - loss: 0.0651 - precision: 1.0000 - recall: 0.9900\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.0610 - precision: 1.0000 - recall: 0.9900\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 167us/step - loss: 0.0572 - precision: 1.0000 - recall: 0.9900\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0557 - precision: 1.0000 - recall: 1.0000\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 149us/step - loss: 0.0531 - precision: 1.0000 - recall: 0.9900\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 175us/step - loss: 0.0511 - precision: 1.0000 - recall: 0.9900\n",
      "100/100 [==============================] - 0s 299us/step\n",
      "N\t100\n",
      "P@1\t1.000\n",
      "R@1\t1.000\n",
      "Predict: [('__label__2', 0.9993414), ('__label__7', 0.00011743841), ('__label__9', 0.00010072714)]\n"
     ]
    }
   ],
   "source": [
    "data_path = '../classifier_data.txt'\n",
    "model_path = '/tmp/FastText.bin.gz'\n",
    "text = '''birchas chaim , yeshiva birchas chaim is a orthodox jewish mesivta high school in \n",
    "lakewood township new jersey . it was founded by rabbi shmuel zalmen stein in 2001 after his \n",
    "father rabbi chaim stein asked him to open a branch of telshe yeshiva in lakewood . \n",
    "as of the 2009-10 school year the school had an enrollment of 76 students and 6 . 6 classroom \n",
    "teachers ( on a fte basis ) for a studentâ€“teacher ratio of 11 . 5 1 .'''\n",
    "\n",
    "model = train_supervised(data_path, wordNgrams=2, lr=0.01, epoch=50, minCount=5)\n",
    "model.save_model(model_path)\n",
    "model2 = load_model(model_path)\n",
    "model2.test(data_path)\n",
    "\n",
    "print('Predict:', model2.predict(text, k=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
